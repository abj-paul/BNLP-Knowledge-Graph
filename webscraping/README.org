* About
Use arxivScraper.py to download papers' metadata on any topic.

Installation process: First, we need to run the docker for scientific pdf parser.
#+begin_src bash
  python3 -m venv scrapeEnv
  source scrapeEnv/bin/activate
  pip install -r requirements.txt
  cd scrapeEnv/bin
  ./serve_grobid.sh
#+end_src

Now we will pull some pdfs from arxivscraper.
#+begin_src bash
  python3 arxivScraper.py
#+end_src

After that, we can use scipdf to parse those PDFs into json format.
#+begin_src bash
  
#+end_src

* Temp 
suggest me resources. I want to cluster papers with similar topics. A paper can belong to multiple cluster so the process is much like topic modeling as well.

 here are 50 topics in natural language processing:

    Stemming
    Part-of-Speech (POS) Tagging
    Named Entity Recognition (NER)
    Sentiment Analysis
    Machine Translation
    Dependency Parsing
    Coreference Resolution
    Semantic Role Labeling (SRL)
    Text Summarization
    Language Modeling
    Text Classification
    Entity Linking
    Information Extraction
    Question Answering
    Text Generation
    Document Clustering
    Text Normalization
    Emotion Analysis
    Opinion Mining
    Core NLP Pipeline
    Syntactic Parsing
    Dependency Trees
    Named Entity Disambiguation
    Semantic Parsing
    Speech Recognition
    Discourse Analysis
    Paraphrase Detection
    Knowledge Base Population
    Text Alignment
    Document Classification
    Text-to-Speech Synthesis
    Dialog Systems
    Cross-lingual Information Retrieval
    Multi-document Summarization
    Code-Switching Detection
    Semantic Similarity
    Error Detection and Correction
    Topic Modeling
    Information Retrieval
    Named Entity Normalization
    Morphological Analysis
    Anaphora Resolution
    Lexical Semantics
    Machine Reading Comprehension
    Machine Assisted Translation
    Sentiment Classification
    Argument Mining
    Domain Adaptation
    Automatic Text Simplification
    Semantic Search
